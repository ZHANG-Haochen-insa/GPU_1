# 练习 3.1 测试报告 (Exercise 3.1 Test Report)

## 1. 目标 (Objective)

本次测试旨在评估练习3.1中实现的初步PyCUDA代码的性能和行为。主要目标包括：
1.  将PyCUDA实现的性能与Numpy（本地）和PyOpenCL进行比较。
2.  验证并复现教程中描述的、基于`threadIdx.x`的“不正确”实现的内在局限性。

## 2. 测试方法 (Methodology)

我们运行了`code/3.1/MySteps_3.py`脚本，并使用了一系列不同的向量大小（从32到32768）作为输入。对于每次运行，我们记录了三种实现（Numpy, OpenCL, CUDA）的性能速率（以“百万元素/秒”或 M-elem/s 为单位）。

## 3. 测试结果 (Results)

下表总结了从测试日志中收集到的性能数据：

| 向量大小 (Size) | Numpy 速率 (M-elem/s) | OpenCL 速率 (M-elem/s) | CUDA 速率 (M-elem/s) |
| :-------------- | :-------------------- | :--------------------- | :------------------- |
| 32              | 2.63                  | 0.00                   | 0.00                 |
| 64              | 7.90                  | 0.00                   | 0.01                 |
| 128             | 11.93                 | 0.00                   | 0.01                 |
| 256             | 29.02                 | 0.00                   | 0.02                 |
| 512             | 42.95                 | 0.00                   | 0.05                 |
| 1024            | 104.76                | 0.00                   | 0.08                 |
| 2048            | 182.76                | 0.01                   | **失败 (Failed)**    |
| 4096            | 464.32                | 0.02                   | **失败 (Failed)**    |
| 8192            | 818.09                | 0.03                   | **失败 (Failed)**    |
| 16384           | 1676.08               | 0.07                   | **失败 (Failed)**    |
| 32768           | 2694.88               | 0.14                   | **失败 (Failed)**    |

## 4. 分析 (Analysis)

### 4.1. 性能表现

对于所有成功执行的测试（向量大小 ≤ 1024），**Numpy的性能远超OpenCL和CUDA**。这完全符合预期。原因在于：
- **计算密度低**：简单的向量加法不涉及复杂的计算。
- **开销过大**：对于GPU实现（OpenCL和CUDA），将数据从主机内存(CPU)复制到设备内存(GPU)、启动内核、再将结果复制回主机的开销，远远超过了GPU在计算本身上节省的时间。

因此，在这种场景下，使用GPU不仅没有带来任何性能提升，反而导致了显著的性能下降。

### 4.2. CUDA 实现的失败

测试结果清晰地表明，当向量大小**严格大于1024**时，CUDA实现必定失败。测试日志中记录的错误是：
`cuFuncSetBlockShape failed: invalid argument`

这个错误的原因正如教程中所解释的：
- 代码中使用了 `block=(int(a_np.size), 1, 1)` 来定义内核的线程块布局。
- 这意味着我们试图在一个线程块（Block）中启动 `a_np.size` 个线程。
- 而单个CUDA线程块中的线程数量存在硬件上限，这个上限**通常是1024**。
- 当请求的线程数（例如2048）超过这个上限时，CUDA驱动会拒绝该请求并返回一个“无效参数”的错误。

## 5. 结论 (Conclusion)

本次测试成功地验证了练习3.1的目标：
1.  证明了对于算术复杂度低的操作，GPU的开销使其性能远不如现代CPU上的Numpy实现。
2.  成功复现了由于不正确地使用CUDA并行层次（仅使用`threadIdx.x`而超出单个Block的线程数限制）而导致的执行失败。

这个结果强调了理解并正确利用CUDA的 **线程块（Blocks）** 和 **线程（Threads）** 这两个并行层次的重要性。下一步的练习（3.2）将通过引入`blockIdx.x`来使用多个线程块，从而解决这个根本性的限制。
