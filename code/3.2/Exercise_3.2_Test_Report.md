# 练习 3.2 测试报告

## 1. 目标 (Objective)

本次测试旨在评估练习3.2中修正后的PyCUDA代码。该代码通过为每个元素分配一个独立的线程块（Block）来规避单个线程块的线程数限制。主要目标包括：
1.  验证新的CUDA实现是否能成功处理大于1024个元素的大规模向量。
2.  在更大的数据规模上，重新对Numpy、OpenCL和修正后的CUDA三者性能进行比较。

## 2. 测试方法 (Methodology)

我们运行了位于 `code/3.2/` 目录下的 `MySteps_4.py` 脚本，并使用了一系列从32,768到16,777,216的大规模向量作为输入。对于每次运行，我们记录了三种实现的性能速率（以“百万元素/秒”或 M-elem/s 为单位）。

## 3. 测试结果 (Results)

下表总结了从测试日志中收集到的性能数据：

| 向量大小 (Size) | Numpy 速率 (M-elem/s) | OpenCL 速率 (M-elem/s) | CUDA 速率 (M-elem/s) |
| :-------------- | :-------------------- | :--------------------- | :------------------- |
| 32,768          | 2545.17               | 0.14                   | 0.11                 |
| 65,536          | 4294.97               | 0.27                   | 5.52                 |
| 131,072         | 4196.61               | 0.57                   | 9.52                 |
| 262,144         | 6704.34               | 1.09                   | 12.55                |
| 524,288         | 6745.47               | 2.12                   | 36.29                |
| 1,048,576       | 6346.39               | 4.32                   | 75.73                |
| 2,097,152       | 4627.09               | 8.82                   | 136.00               |
| 4,194,304       | 2739.79               | 17.50                  | 176.79               |
| 8,388,608       | 2441.49               | 33.28                  | 274.55               |
| 16,777,216      | 2352.92               | 62.61                  | 311.00               |

## 4. 分析 (Analysis)

### 4.1. 实现的成功

最重要的一点是，**修正后的CUDA实现成功处理了所有测试的向量大小**，包括远超1024的规模。这证明了通过将并行化任务分配到多个线程块（Grid维度）而不是单个线程块内部（Block维度），我们成功地绕过了硬件对单个线程块中线程数量的限制。

### 4.2. 性能表现

-   **Numpy 仍然最快**：尽管数据规模显著增大，对于向量加法这种简单的、受限于内存带宽的操作，Numpy（底层由高效的C语言库实现）的性能依然是三者中最高的。
-   **CUDA vs OpenCL**：在修正后，CUDA的性能表现远超OpenCL。随着向量大小的增加，CUDA的性能优势愈发明显。在最大测试规模（约16M元素）下，CUDA的速率大约是OpenCL的5倍（311.00 vs 62.61 M-elem/s）。这表明在这种“一个线程处理一个元素”的模式下，PyCUDA的实现或CUDA驱动的开销管理比PyOpenCL更高效。
-   **可扩展性**：随着数据量的增加，GPU实现的性能（M-elem/s）稳步提升。这说明GPU架构在这种大规模并行任务中，能够通过分摊固定开销（如内核启动、内存拷贝）到更多的计算单元上而展现出更好的可扩展性。

## 5. 结论 (Conclusion)

练习3.2的测试是成功的。我们验证了通过利用线程块（Blocks）进行并行化，可以构建一个能够处理任意大小数据的、可扩展的CUDA程序。

然而，尽管GPU实现的可扩展性得到了验证，但它们的绝对性能对于简单的向量加法来说，仍然远不及在CPU上运行的Numpy。这再次凸显了**算术复杂度**的重要性：为了真正发挥GPU的强大计算能力，我们需要让每个线程执行更多的计算任务，而不仅仅是一次加法。

这为练习3.3——增加内核的算术复杂度以观察性能变化——铺平了道路。
